import csv
import os
import sys

HERE = os.path.dirname(os.path.realpath(__file__))

sys.path.append(os.path.join(HERE, "examples_dir"))
sys.path.append(os.path.join(HERE, "fairseq"))
sys.path.append(os.path.join(HERE, "GENRE/genre"))

from time import time
from genre.trie import Trie
import pandas as pd
import re
from wikimapper import WikiMapper
from genre.fairseq_model import GENRE
from genre.entity_linking import (
    get_end_to_end_prefix_allowed_tokens_fn_fairseq as get_prefix_allowed_tokens_fn,
)
from genre.utils import get_entity_spans_fairseq as get_entity_spans

INPUT_FILE_PATH = os.path.join(HERE, "../data/artist_artwork_sentence.csv")
OUTPUT_FILE_PATH = os.path.join(HERE, "../data/artists_with_wikidata_url.csv")

WIKIMAPPER_INDEX_PATH = os.path.join(HERE, "../data/index_enwiki-latest.db")


def text_to_id(text):
    if text is None or type(text) != str:
        return [None]

    wikipedia_title_match = re.search(
        r"(?<=\[ ).*?(?= \])", text
    )  # between brackets "[", "]"
    entity_mention_match = re.search(
        r"(?<=\{ ).*?(?= \})", text
    )  # between brackets "{", "}"
    if wikipedia_title_match is None or entity_mention_match is None:
        return [None]

    wikipedia_title = wikipedia_title_match.group()
    entity_mention = entity_mention_match.group()

    # first try to map the retrieved Wikipedia title and then the entity mention
    wikidata_id = mapper.title_to_id(wikipedia_title.replace(" ", "_"))
    if wikidata_id is None:
        wikidata_id = mapper.title_to_id(entity_mention.replace(" ", "_"))

    return [wikidata_id]


if __name__ == "__main__":
    genre_model = GENRE.from_pretrained(
        os.path.join(HERE, "fairseq_e2e_entity_linking_wiki_abs")
    ).eval()

    mapper = WikiMapper(WIKIMAPPER_INDEX_PATH)

    artist_sentence_df = pd.read_csv(
        INPUT_FILE_PATH, escapechar="\\", doublequote=False
    )

    artist_sentence_df = artist_sentence_df.dropna()

    sentences = list(artist_sentence_df["sentence"].values)

    # Add spaces in the front sentences and entity mentions for GENRE
    sentences = [f" {sentence.strip()}" for sentence in sentences]
    entity_mentions = [
        f" {sentence.split(':')[0].strip()}" for sentence in sentences
    ]  # use as mentions the artist names generated by the LLM because they may be slightly different

    print("Running GENRE...")

    entity_wikidata_urls = []
    for i, (sentence, entity_mention) in enumerate(zip(sentences, entity_mentions)):
        prefix_allowed_tokens_fn = get_prefix_allowed_tokens_fn(
            genre_model,
            [sentence],
            mention_trie=Trie([genre_model.encode(entity_mention)[1:].tolist()]),
        )

        try:
            linked_sentences_info = genre_model.sample(
                [sentence],
                prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,
                text_to_id=text_to_id,
            )
        except RecursionError:
            print(
                f'Recursion error for sentence "{sentence}" and entity mention "{entity_mention}"'
            )
            entity_wikidata_urls.append("")
            continue

        entity_wikidata_ids = []
        entity_wikidata_url = ""
        for linking_info_json in linked_sentences_info[0]: # only one sentence given
            if len(linking_info_json["id"]) == 0 or not linking_info_json["id"][0]:
                continue

            wikidata_id = linking_info_json["id"][0]
            if wikidata_id in entity_wikidata_ids:
                continue

            entity_wikidata_ids.append(wikidata_id)

            if len(entity_wikidata_url) > 0:
                entity_wikidata_url += " "
            entity_wikidata_url += f"https://www.wikidata.org/wiki/{wikidata_id}"

        entity_wikidata_urls.append(entity_wikidata_url)

    artist_sentence_df["wikidata_url"] = entity_wikidata_urls
    artist_sentence_df.to_csv(
        OUTPUT_FILE_PATH,
        quoting=csv.QUOTE_NONNUMERIC,
        escapechar="\\",
        doublequote=False,
        index=False,
    )
